## 文本分类数据不平衡

在对客户录音文本进行分类过程中，初始使用训练集测试准确率为95%，到实际数据上进行测试时，出现了准确率急速下降，变成0.008。这是因为在生成训练集和测试集时为了模型更好拟合，使用了过采样，使两种类别的比例近似为1:1。而当在实际数据上进行测试时，（1）由于数据的变动，导致模型的准确率可能会下降。（2）由于0,1分类中，类型0的文本是类型1的文本的1600多倍，导致在识别类型1的数目不变得情况下，类型0的误识别数目变为原来的1600多倍。在两者综合影响下准确率就变成一个接近0的极小数。

在这种情况下需要考虑实际应用场景，根据应用场景做准确率或召回率单向优化，如果确实需要准确率和召回率都保持较高水平，对人工进行完全替代，需要多个环节保持较高水平（音转字水平和标签，模型等），下面针对算法方面介绍一下提升方法。

解决方案：

1. 考虑通过使用异常值发现算法（Isolation Forest、one_class的svm、EllipticEnvelope），文本分类通常维度都较高，在高维空间下，文本向量相对总是稀疏，所以较难有效的发现异常值。考虑先进行降维，在进行异常值发现。

2. 对向量特征进行降维。
   1. 考虑通过pca(pca无法有效的降低tf-idf向量的维度，或许是因为tf-idf中的维度词，本身相关性并不大)方法进行降维，再用降维后的数据进行异常值发现。直接使用pca进行降维效果不好，通过放缩tf-idf的值（对tf-idf做平方或立方运算），使得大的值更大，小的值更小，此时使用pca降维会取得更好的降维效果。此时采用pca会取得更好的效果，或许是因为放缩导致tf-idf值小的更接近0，使得大的值产生更大影响，（比如6000维的值，放缩后，可能4000维变成近似0的数）故pca效果提高。
   2. 使用聚类进行降维，通过将数据聚类成n个类别，使用kmeans或者其他聚类算法，可以得到向量归属每个类别的概率，从而将数据降维到n维。
   3. 使用皮尔森相关系数或者卡方检验进行降维，通过将与分类相关性低的列去掉，来达到降维的目的。
   4. 使用随机森林，gbdt等的变量重要性度量，来去掉重要性低的变量进行降维。
3. 考虑进行人为构建boosting，将分类错误的文本和类型1的全部文本重新构建模型，预测后再次将预测错误的与类型1的文本合并构建新的模型，通过这种迭代来达到提高准确率的效果。