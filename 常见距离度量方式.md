# 常见距离度量方式

## 数值属性的距离度量

### 闵可夫斯基距离（MInkowski distance）

$\Large dist_{mk}(x_i,x_j)=(\sum_{u=1}^{n}|x_{iu}-x_{ju}|^p)^{\frac{1}{p}}$

#### 欧式距离

当p=2时，闵可夫斯基距离即为欧式距离（Euclidean distance），欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。

$\Large dist_{ed}(x_i,x_j)=||x_i-x_j||_2=\sqrt{\sum_{u=1}^{n}|x_{iu}-x_{ju}|^2}$

#### 曼哈顿距离

当p=1时，闵可夫斯基距离即为曼哈顿距离（Manhattan distance），又名城市街区距离，计算两个地点的驾驶距离。

$\Large dist_{man}(x_i,x_j)=||x_i-x_j||_1=\sum_{u=1}^{n}|x_{iu}-x_{ju}|$

#### 切比雪夫距离

当$p=\infty $时,闵可夫斯基距离即为切比雪夫距离（ Chebyshev Distance）。 国际象棋玩过么？国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？自己走走试试。你会发现最少步数总是max(| x2-x1 | , | y2-y1 | ) 步。有一种类似的一种距离度量方法叫切比雪夫距离。

$\Large dist_{cd}(x_i,x_j)=\lim_{p \to \infty}(\sum_{u=1}^{n}|x_{iu}-x_{ju}|^p)^{\frac{1}{p}}=\max\limits_{1\le u\le n}(|x_{iu}-x_{ju}|)$

### 马氏距离

有M个样本向量X1~Xm，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：

$\Large dist_{md}(X)=\sqrt{(X-\mu)^TS^{-1}(X-\mu)}$

 而其中向量$X_i$与$X_j$之间的马氏距离定义为：

$\Large dist_{md}(X_i,X_j)=\sqrt{(X_i-X_j)^TS^{-1}(X_i-X_j)}$

马氏距离的优点：

马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。 

马氏距离的缺点：

马氏距离夸大了变化微小的变量的作用。

### 余弦距离

> 在sklearn中有余弦距离和余弦相似度，余弦相似度即为下面的公式，余弦距离定义为1-余弦相似度。

$$\Large  Cos_\theta=\frac{\sum_1^n(x_i y_i)}{\sqrt{\sum_1^n x_i^2}\sqrt{\sum_1^n y_i^2}}$$

余弦值的范围在[-1,1]之间，值越趋近于1，代表两个向量的方向越接近；越趋近于-1，他们的方向越相反；接近于0，表示两个向量近乎于正交。

当在做客户评分相似度比较时，不能直接比较两者的评分，需要比较两者对于均值的偏移值。如下例子：

虽然余弦相似度对个体间存在的偏见可以进行一定的修正，但是因为只能分辨个体在维之间的差异，没法衡量每个维数值的差异，会导致这样一个情况：比如用户对内容评分，5分制，X和Y两个用户对两个内容的评分分别为(1,2)和(4,5)，使用余弦相似度得出的结果是0.98，两者极为相似，但从评分上看X似乎不喜欢这2个内容，而Y比较喜欢，余弦相似度对数值的不敏感导致了结果的误差，需要修正这种不合理性，就出现了调整余弦相似度，即所有维度上的数值都减去一个均值，比如X和Y的评分均值都是3，那么调整后为(-2,-1)和(1,2)，再用余弦相似度计算，得到-0.8，相似度为负值并且差异不小，但显然更加符合现实。