## 计算机视觉介绍



相比于单一的传统机器学习算法，深度学习算法由多样化的模型组成；这是由于神经网络在构建一个完整的端到端的模型时所提供的灵活性。

神经网络有时可比作乐高块，借助想象力你几乎可以用它建构从简单到复杂的任何结构。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/97799640.jpeg)

我们可以把高级架构定义为一个具有良好记录的成功模型；这主要见于挑战赛中，比如 ImageNet，其中你的任务是借助给定的数据解决图像识别等问题。

正如下文所描述的每一个架构，其中每一个都与常见的模型有细微不同，在解决问题时这成了一种优势。这些架构同样属于「深度」模型的范畴，因此有可能比浅层模型表现更好。

### 计算机视觉任务的类型

本文主要聚焦于计算机视觉，因此很自然地描述了计算机视觉任务的分类。顾名思义，计算机视觉即通过创建人工模型来模拟本由人类执行的视觉任务。其本质是人类的感知与观察是一个过程，它可在人工系统中被理解和实现。

计算机视觉任务的主要类型如下：

- 物体识别／分类：在物体识别中，给出一张原始图像，你的任务是识别出该图像属于哪个类别。
- 分类+定位：如果图像中只有一个物体，你的任务是找到该物体在图像中的位置，一个更专业的称谓是定位。
- 物体检测：在物体检测中，你的任务是找到图像中多个物体的各自位置。这些物体可能属于同一类别，或者各自不同。
- 图像分割：图像分割是一个稍微复杂的任务，其目标是将每一个像素映射到正确的分类。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/37084640-2.jpeg)

### 物体识别/分类

#### AlexNet

AlexNet 是首个深度架构，它由深度学习先驱 Geoffrey Hinton 及其同僚共同引入。AlexNet 是一个简单却功能强大的网络架构，为深度学习的开创性研究铺平了道路。下图是论文作者提出架构的示图。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/70912640-3.jpeg)

如图所示，分解后的 AlexNet 像是一个简单的架构，卷积层和池化层层叠加，最上层是全连接层。这是一个非常简单的架构，其早在 80 年代就已被概念化。但是该模型的突出特征是其执行任务的规模与使用 GPU 进行训练。20 世纪 80 年代，训练神经网络使用的是 CPU，而 AlexNet 借助 GPU 将训练提速了 10x。

论文：ImageNet Classification with Deep Convolutional Neural Networks

- 链接：https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf
- 代码实现：https://gist.github.com/JBed/c2fb3ce8ed299f197eff

#### VGG Net

VGG 网络由牛津可视化图形组（Visual Graphics Group）开发，因此其名称为 VGG。该网络的特点是金字塔形，与图像最近的底层比较宽，而顶层很深。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/07746640-4.jpeg)

如上图所示，VGG 包含池化层之后的卷积层，池化层负责使层变窄。他们在论文中提出多个此类网络，不同之处在于架构深度的变化。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/30767640-5.jpeg)

VGG 的优势：

- 适合在特定任务上进行基准测试。
- VGG 的预训练网络可在互联网上免费获取，因此被广泛用于各种应用。

另一方面，它的主要缺陷在于如果从头训练，则过程缓慢。即使在性能很好的 GPU 上，也需要一周多的时间才能完成训练。

论文：Very Deep Convolutional Networks for Large-Scale Image Recognition

- 链接：https://arxiv.org/abs/1409.1556
- 代码实现：https://github.com/fchollet/keras/blob/master/keras/applications/vgg16.py

#### GoogleNet

GoogleNet（或 Inception 网络）是谷歌研究者设计的一种架构。GoogleNet 是 ImageNet 2014 的冠军，是当时最强大的模型。

该架构中，随着深度增加（它包含 22 层，而 VGG 只有 19 层），研究者还开发了一种叫作「Inception 模块」的新型方法。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/71632image-6.png)

如上图所示，它与我们之前看到的序列架构发生了很大改变。单个层中出现了多种「特征提取器（feature extractor）」。这间接地改善了该网络的性能，因为该网络在训练过程中有多个选项可以选择，来解决该任务。它可以选择与输入进行卷积，也可以直接将其池化。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/90724640-7.jpeg)

最终架构包括堆叠在一起的多个 inception 模块。GoogleNet 的训练过程也有稍许不同，即最上层有自己的输出层。这一细微差别帮助模型更快地进行卷积，因为模型内存在联合训练和层本身的并行训练。

GoogleNet 的优势在于：

- GoogleNet 训练速度比 VGG 快。
- 预训练 GoogleNet 的规模比 VGG 小。VGG 模型大于 500 MB，而 GoogleNet 的大小只有 96MB。

GoogleNet 本身没有短期劣势，但是该架构的进一步改变使模型性能更佳。其中一个变化是 Xception 网络，它增加了 inception 模块的发散极限（我们可以从上图中看到 GoogleNet 中有 4 个 inception 模块）。现在从理论上讲，该架构是无限的（因此又叫极限 inception！）。

论文：Rethinking the Inception Architecture for Computer Vision

- 链接：https://arxiv.org/abs/1512.00567
- 代码实现：https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py

#### ResNet

ResNet 是一个妖怪般的架构，让我们看到了深度学习架构能够有多深。残差网络（ResNet）包含多个后续残差模块，是建立 ResNet 架构的基础。下图是残差模块的表示图：

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/23662640-8.jpeg)

简言之，一个残差模块有两个选择：完成输入端的一系列函数，或者跳过此步骤。

类似于 GoogleNet，这些残差模块一个接一个地堆叠，组成了完整的端到端网络。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/41356640-9.jpeg)

ResNet 引入的新技术有：

- 使用标准的 SGD，而非适应性学习技术。它联通一个合理的初始化函数（保持训练的完整性）做到的这一点。
- 输入预处理的变化，输入首先被区分到图像块中，然后输送到网络中。

ResNet 主要的优势是数百，甚至数千的残差层都能被用于创造一个新网络，然后训练。这不同于平常的序列网络，增加层数量时表现会下降。

论文：Deep Residual Learning for Image Recognition

- 链接：https://arxiv.org/abs/1512.03385
- 代码实现：https://github.com/fchollet/keras/blob/master/keras/applications/resnet50.py

#### ResNeXt

ResNeXt 据说是解决目标识别问题的最先进技术。它建立在 inception 和 resnet 的概念上，并带来改进的新架构。下图是对 ResNeXt 模块中的残差模块的总结。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/71728640-10.jpeg)

*图 1. 左：ResNet 块。右：基数=32 的 ResNeXt 块，复杂度大致相同。层显示为（# in channels, filter size, # out channels）。*

论文：Aggregated Residual Transformations for Deep Neural Networks

- 链接：https://arxiv.org/pdf/1611.05431.pdf
- 代码实现：https://github.com/titu1994/Keras-ResNeXt

#### SqueezeNet

SqueeNet 架构是在移动平台这样的低宽带场景中极其强大的一种架构。这种架构只占用 4.9 MB 的空间，而 Inception 架构大小为 100MB。这种巨大的差距由一种名为 Fire Module 的特殊结构引起。下图是 Fire Module 的表示图：

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/92883640-14.jpeg)

SqueezeNet 的完整架构如下：

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/12658640-15.jpeg)

论文：SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE

- 链接：https://arxiv.org/abs/1602.07360
- 代码实现：https://github.com/rcmalli/keras-squeezenet

### 分类+定位

### 物体检测

#### RCNN

RCNN算法分为4个步骤 
\- 一张图像生成1K~2K个**候选区域** 
\- 对每个候选区域，使用深度网络**提取特征** 
\- 特征送入每一类的SVM **分类器**，判别是否属于该类 
\- 使用回归器**精细修正**候选框位置 
![这里写图片描述](http://img.blog.csdn.net/20160405215259014)

##### 候选区域生成

使用了Selective Search[1](http://blog.csdn.net/shenxiaolu1984/article/details/51066975#fn:1)方法从一张图像生成约2000-3000个候选区域。基本思路如下： 
\- 使用一种过分割手段，将图像分割成小区域 
\- 查看现有小区域，合并**可能性最高**的两个区域。重复直到整张图像合并成一个区域位置 
\- 输出所有曾经存在过的区域，所谓候选区域

候选区域生成和后续步骤相对独立，实际可以使用任意算法进行。

##### 合并规则

优先合并以下四种区域： 

\- 颜色（颜色直方图）相近的 
\- 纹理（梯度直方图）相近的 
\- 合并后总面积小的 
\- 合并后，总面积在其BBOX中所占比例大的

第五条，保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域。

> 例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -> abcd-efgh -> abcdefgh。 
> 不好的合并方法是：ab-c-d-e-f-g-h ->abcd-e-f-g-h ->abcdef-gh -> abcdefgh。

第六条，保证合并后形状规则。

> 例：左图适于合并，右图不适于合并。 
> ![这里写图片描述](http://img.blog.csdn.net/20160405212106908)

上述六条规则只涉及区域的颜色直方图、纹理直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。

##### 多样化与后处理

为尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（RGB,HSV,Lab等）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。

作者提供了Selective Search的[源码](http://koen.me/research/selectivesearch/)，内含较多.p文件和.mex文件，难以细查具体实现。

##### 特征提取

###### 预处理

使用深度网络提取特征之前，首先把候选区域归一化成同一尺寸227×227。 
此处有一些细节可做变化：外扩的尺寸大小，形变时是否保持原比例，对框外区域直接截取还是补灰。会轻微影响性能。

###### 预训练

**网络结构** 
基本借鉴Hinton 2012年在Image Net上的分类网络[2](http://blog.csdn.net/shenxiaolu1984/article/details/51066975#fn:2)，略作简化[3](http://blog.csdn.net/shenxiaolu1984/article/details/51066975#fn:3)。 
![这里写图片描述](http://img.blog.csdn.net/20160405214721512) 
此网络提取的特征为4096维，之后送入一个4096->1000的全连接(fc)层进行分类。 
学习率0.01。

**训练数据** 
使用ILVCR 2012的全部数据进行训练，输入一张图片，输出1000维的类别标号。

###### 调优训练

**网络结构** 
同样使用上述网络，最后一层换成4096->21的全连接网络。 
学习率0.001，每一个batch包含32个正样本（属于20类）和96个背景。

**训练数据** 
使用PASCAL VOC 2007的训练集，输入一张图片，输出21维的类别标号，表示20类+背景。 
考察一个候选框和当前图像上所有标定框重叠面积最大的一个。如果重叠比例大于0.5，则认为此候选框为此标定的类别；否则认为此候选框为背景。

##### 类别判断

**分类器** 
对每一类目标，使用一个线性SVM二类分类器进行判别。输入为深度网络输出的4096维特征，输出是否属于此类。 
由于负样本很多，使用hard negative mining方法。 
**正样本** 
本类的真值标定框。 
**负样本** 
考察每一个候选框，如果和本类所有标定框的重叠都小于0.3，认定其为负样本

##### 位置精修

目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 
**回归器** 
对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 
输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 
**训练样本** 
判定为本类的候选框中，和真值重叠面积大于0.6的候选框。

##### 结果

论文发表的2014年，DPM已经进入瓶颈期，即使使用复杂的特征和结构得到的提升也十分有限。本文将深度学习引入检测领域，一举将PASCAL VOC上的检测率**从35.1%提升到53.7%**。 
本文的前两个步骤（候选区域提取+特征提取）与待检测类别无关，可以在不同类之间共用。这两步在GPU上约需13秒。 
同时检测多类时，需要倍增的只有后两步骤（判别+精修），都是简单的线性运算，速度很快。这两步对于100K类别只需10秒。

#### Fast RCNN

继2014年的RCNN之后，Ross Girshick在15年推出Fast RCNN，构思精巧，流程更为紧凑，大幅提升了目标检测的速度。在Github上提供了[源码](https://github.com/rbgirshick/fast-rcnn)。

同样使用最大规模的网络，Fast RCNN和RCNN相比，训练时间从84小时减少为9.5小时，**测试时间从47秒减少为0.32秒**。在PASCAL VOC 2007上的准确率相差无几，约在66%-67%之间.

##### 思想

###### 基础：RCNN

简单来说，RCNN使用以下四步实现目标检测： 
a. 在图像中确定约1000-2000个**候选框** 
b. 对于每个候选框内图像块，使用**深度网络**提取**特征** 
c. 对候选框中提取出的特征，使用**分类器**判别是否属于一个特定类 
d. 对于属于某一特征的候选框，用**回归器**进一步调整其**位置** 
更多细节可以参看[这篇博客](http://blog.csdn.net/shenxiaolu1984/article/details/51066975)。

###### 改进：Fast RCNN

Fast RCNN方法解决了RCNN方法三个问题：

###### **问题一：测试时速度慢** 

RCNN一张图像内候选框之间大量重叠，提取特征操作冗余。 
本文将**整张图像**归一化后**直接送入深度网络**。在邻接时，才加入候选框信息，在末尾的少数几层处理每个候选框。

###### **问题二：训练时速度慢** 

原因同上。 
在训练时，本文先将一张图像送入网络，**紧接着送入**从这幅图像上提取出的**候选区域**。这些候选区域的前几层特征不需要再重复计算。

###### **问题三：训练所需空间大** 

RCNN中独立的分类器和回归器需要大量特征作为训练样本。 
本文把类别判断和位置精调**统一用深度网络实现**，不再需要额外存储。

以下按次序介绍三个问题对应的解决方法。

##### 特征提取网络

###### 基本结构

图像归一化为224×224直接送入网络。

前五阶段是基础的conv+relu+pooling形式，在第五阶段结尾，输入P个候选区域（图像序号×1+几何位置×4，序号用于训练）？。 
![这里写图片描述](http://img.blog.csdn.net/20160411214438672)

> 注：文中给出了大中小三种网络，此处示出最大的一种。三种网络基本结构相似，仅conv+relu层数有差别，或者增删了norm层。

###### roi_pool层的测试(forward)

roi_pool层将每个候选区域均匀分成M×N块，对每块进行max pooling。将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。 
![这里写图片描述](http://img.blog.csdn.net/20160411113225519)

###### roi_pool层的训练(backward)

首先考虑普通max pooling层。设xi为输入层的节点，yj为输出层的节点。 

![](.\cvpic\0.bmp)

其中判决函数δ(i,j)表示i节点是否被j节点选为最大值输出。不被选中有两种可能：xi不在yj范围内，或者xi不是最大值。

对于roi max pooling，一个输入节点可能和多个输出节点相连。设xi为输入层的节点，yrj为第r个候选区域的第j个输出节点。 
![这里写图片描述](http://img.blog.csdn.net/20160411221426650) 

![](.\cvpic\1.bmp)

判决函数δ(i,r,j)表示i节点是否被候选区域r的第j个节点选为最大值输出。代价对于xi的梯度等于所有相关的后一层梯度之和。

##### 网络参数训练

###### 参数初始化

网络除去末尾部分如下图，在ImageNet上训练1000类分类器。结果参数作为相应层的初始化参数。 
![这里写图片描述](http://img.blog.csdn.net/20160411214618571) 
其余参数随机初始化。

###### 分层数据

在调优训练时，每一个mini-batch中首先加入N张完整图片，而后加入从N张图片中选取的R个候选框。这R个候选框可以复用N张图片前5个阶段的网络特征。 
实际选择N=2， R=128。

###### 训练数据构成

N张完整图片以50%概率水平翻转。 
R个候选框的构成方式如下：

| 类别   | 比例   | 方式                      |
| ---- | ---- | ----------------------- |
| 前景   | 25%  | 与某个真值重叠在[0.5,1]的候选框     |
| 背景   | 75%  | 与真值重叠的最大值在[0.1,0.5)的候选框 |

##### 分类与位置调整

###### 数据结构

第五阶段的特征输入到两个并行的全连层中（称为multi-task）。 
![这里写图片描述](http://img.blog.csdn.net/20160411154103099)
**cls_score层**用于分类，输出K+1维数组p，表示属于K类和背景的概率。 
**bbox_prdict层**用于调整候选区域位置，输出4*K维数组t，表示分别属于K类时，应该平移缩放的参数。

###### 代价函数

loss_cls层评估分类代价。由真实分类u对应的概率决定： 

![](.\cvpic\2.bmp)

loss_bbox评估检测框定位代价。比较真实分类对应的预测参数tu和真实平移缩放参数为v的差别： 

![](.\cvpic\3.bmp)

g为Smooth L1误差，对outlier不敏感： 

![](.\cvpic\4.bmp)

总代价为两者加权和，如果分类为背景则不考虑定位代价： 

![](.\cvpic\5.bmp)

> 源码中bbox_loss_weights用于标记每一个bbox是否属于某一个类

###### 全连接层提速

分类和位置调整都是通过全连接层(fc)实现的，设前一级数据为x后一级为y，全连接层参数为W，尺寸u×v。一次前向传播(forward)即为： 

![](.\cvpic\6.bmp)

计算复杂度为u×v。

将W进行SVD分解，并用前t个特征值近似： 

![](.\cvpic\7.bmp)

原来的前向传播分解成两步： 

![](.\cvpic\8.bmp)

计算复杂度变为u×t+v×t。 

在实现时，相当于把一个全连接层拆分成两个，中间以一个低维数据相连。 

![](.\cvpic\9.bmp)

> 在github的源码中，这部分似乎没有实现。

##### 实验与结论

实验过程不再详述，只记录结论 
\- 网络末端**同步训练**的分类和位置调整，提升准确度 
\- 使用**多尺度**的图像金字塔，性能几乎没有提高 
- **倍增训练**数据，能够有2%-3%的准确度提升 
  \- 网络直接输出各类概率(**softmax**)，比SVM分类器性能略好 
- **更多候选窗**不能提升性能

#### Faster RCNN (基于区域的 CNN)

论文：Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

- 链接：https://arxiv.org/abs/1506.01497
- 代码实现：https://github.com/yhenon/keras-frcnn

本文是继RCNN[[1](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:1)]，fast RCNN[[2](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:2)]之后，目标检测界的领军人物[Ross Girshick](http://www.cs.berkeley.edu/~rbg/)团队在2015年的又一力作。**简单网络目标检测速度达到17fps**，在PASCAL VOC上准确率为59.9%；复杂网络达到5fps，准确率78.8%。

作者在github上给出了基于[matlab](https://github.com/ShaoqingRen/faster_rcnn)和[python](https://github.com/rbgirshick/py-faster-rcnn)的源码。对Region CNN算法不了解的同学，请先参看这两篇文章：[《RCNN算法详解》](http://blog.csdn.net/shenxiaolu1984/article/details/51066975)，[《fast RCNN算法详解》](http://blog.csdn.net/shenxiaolu1984/article/details/51036677)。

##### 思想

从RCNN到fast RCNN，再到本文的faster RCNN，目标检测的四个基本步骤（候选区域生成，特征提取，分类，位置精修）终于被**统一到一个深度网络框架之内**。所有计算没有重复，完全在GPU中完成，大大提高了运行速度。 
![这里写图片描述](http://img.blog.csdn.net/20160414164536029)

faster RCNN可以简单地看做“区域生成网络(RPN)+fast RCNN“的系统，用区域生成网络代替fast RCNN中的Selective Search方法。本篇论文着重解决了这个系统中的三个问题： 

1. 如何**设计**区域生成网络 
2. 如何**训练**区域生成网络 
3. 如何让区域生成网络和fast RCNN网络**共享特征提取网络**

##### 区域生成网络：结构

基本设想是：在提取好的特征图上，对所有可能的候选框进行判别。由于后续还有位置精修步骤，所以候选框实际比较稀疏。 
![这里写图片描述](http://img.blog.csdn.net/20160415133947737)

###### 特征提取

原始特征提取（上图灰色方框）包含若干层conv+relu，直接套用ImageNet上常见的分类网络即可。本文试验了两种网络：5层的ZF[[3](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:3)]，16层的VGG-16[[4](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:4)]，具体结构不再赘述。 
额外添加一个conv+relu层，输出51*39*256维特征（feature）。

###### 候选区域（anchor）

特征可以看做一个尺度51*39的256通道图像，对于该图像的每一个位置，考虑9个可能的候选窗口：三种面积{1282,2562,5122}×三种比例{1:1,1:2,2:1}。这些候选窗口称为anchors。下图示出51*39个anchor中心，以及9种anchor示例。 
![这里写图片描述](http://img.blog.csdn.net/20160419145709355)

> 在整个faster RCNN算法中，有三种尺度。 
> **原图尺度**：原始输入的大小。不受任何限制，不影响性能。 
> **归一化尺度**：输入特征提取网络的大小，在测试时设置，源码中opts.test_scale=600。anchor在这个尺度上设定。这个参数和anchor的相对大小决定了想要检测的目标范围。 
> **网络输入尺度**：输入特征检测网络的大小，在训练时设置，源码中为224*224。

###### 窗口分类和位置精修

分类层（cls_score）输出每一个位置上，9个anchor属于前景和背景的概率；窗口回归层（bbox_pred）输出每一个位置上，9个anchor对应窗口应该平移缩放的参数。 
对于每一个位置来说，分类层从256维特征中输出属于前景和背景的概率；窗口回归层从256维特征中输出4个平移缩放参数。

就局部来说，这两层是全连接网络；就全局来说，由于网络在所有位置（共51*39个）的参数相同，所以实际用尺寸为1×1的卷积网络实现。

> 实际代码中，将51*39*9个候选位置根据得分排序，选择最高的一部分，再经过Non-Maximum Suppression获得2000个候选结果。之后才送入分类器和回归器。 
> 所以Faster-RCNN和RCNN, Fast-RCNN一样，属于2-stage的检测算法。

##### 区域生成网络：训练

###### 样本

考察训练集中的每张图像： 
a. 对每个标定的真值候选区域，与其重叠比例最大的anchor记为前景样本 
b. 对a)剩余的anchor，如果其与某个标定重叠比例大于0.7，记为前景样本；如果其与任意一个标定的重叠比例都小于0.3，记为背景样本 
c. 对a),b)剩余的anchor，弃去不用。 
d. 跨越图像边界的anchor弃去不用

###### 代价函数

同时最小化两种代价： 
a. 分类误差 
b. 前景样本的窗口位置偏差 
具体参看fast RCNN中的[“分类与位置调整”段落](http://blog.csdn.net/shenxiaolu1984/article/details/51036677#t11)。

###### 超参数

原始特征提取网络使用ImageNet的分类样本初始化，其余新增层随机初始化。 
每个mini-batch包含从一张图像中提取的256个anchor，前景背景样本1:1. 
前60K迭代，学习率0.001，后20K迭代，学习率0.0001。 
momentum设置为0.9，weight decay设置为0.0005。[[5](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:5)]

##### 共享特征

区域生成网络（RPN）和fast RCNN都需要一个原始特征提取网络（下图灰色方框）。这个网络使用ImageNet的分类库得到初始参数W0，但要如何精调参数，使其同时满足两方的需求呢？本文讲解了三种方法。 
![这里写图片描述](http://img.blog.csdn.net/20160415134137682)

###### 轮流训练

a. 从W0开始，训练RPN。用RPN提取训练集上的候选区域 
b. 从W0开始，用候选区域训练Fast RCNN，参数记为W1 
c. 从W1开始，训练RPN… 
具体操作时，仅执行两次迭代，并在训练时冻结了部分层。论文中的实验使用此方法。 
如Ross Girshick在ICCV 15年的讲座Training R-CNNs of various velocities中所述，采用此方法没有什么根本原因，主要是因为”实现问题，以及截稿日期“。

###### 近似联合训练

直接在上图结构上训练。在backward计算梯度时，把提取的ROI区域当做固定值看待；在backward更新参数时，来自RPN和来自Fast RCNN的增量合并输入原始特征提取层。 
此方法和前方法效果类似，但能将训练时间减少20%-25%。[公布的python代码](https://github.com/rbgirshick/py-faster-rcnn)中包含此方法。

###### 联合训练

直接在上图结构上训练。但在backward计算梯度时，要考虑ROI区域的变化的影响。推导超出本文范畴，请参看15年NIP论文[[6](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:6)]。

##### 实验

除了开篇提到的基本性能外，还有一些值得注意的结论

- 与Selective Search方法（黑）相比，当每张图生成的候选区域从2000减少到300时，本文RPN方法（红蓝）的召回率下降不大。说明**RPN方法的目的性更明确**。 
  ![这里写图片描述](http://img.blog.csdn.net/20160415150343559)
- 使用更大的Microsoft COCO库[[7](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fn:7)]训练，直接在PASCAL VOC上测试，准确率提升6%。说明**faster RCNN迁移性良好**，没有over fitting。 
  ![这里写图片描述](http://img.blog.csdn.net/20160415151012671)

------

1. Girshick, Ross, et al. “Rich feature hierarchies for accurate object detection and semantic segmentation.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [↩](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fnref:1)
2. Girshick, Ross. “Fast r-cnn.” Proceedings of the IEEE International Conference on Computer Vision. 2015. [↩](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fnref:2)
3. M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional neural networks,” in European Conference on Computer Vision (ECCV), 2014. [↩](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fnref:3)
4. K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in International Conference on Learning Representations (ICLR), 2015. [↩](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fnref:4)
5. learning rate-控制增量和梯度之间的关系；momentum-保持前次迭代的增量；weight decay-每次迭代缩小参数，相当于正则化。 [↩](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fnref:5)
6. Jaderberg et al. “Spatial Transformer Networks” 
   NIPS 2015 [↩](http://blog.csdn.net/shenxiaolu1984/article/details/51152614#fnref:6)
7. 30万+图像，80类检测库。参看<http://mscoco.org/>。

#### R-FCN

还记得 Fast R-CNN 是如何通过在所有 region proposal 上共享同一个 CNN，来改善检测速度的吗？这也是设计 R-FCN 的一个动机：通过最大化共享计算来提升速度。

R-FCN，或称 Region-based Fully Convolutional Net（基于区域的全卷积网络），可以在每个输出之间完全共享计算。作为全卷积网络，它在模型设计过程中遇到了一个特殊的问题。

一方面，当对一个目标进行分类任务时，我们希望学到模型中的位置不变性（location invariance）：无论这只猫出现在图中的哪个位置，我们都想将它分类成一只猫。另一方面，当进行目标检测任务时，我们希望学习到位置可变性（location variance）：如果这只猫在左上角，那么我们希望在图像左上角这个位置画一个框。所以，问题出现了，如果想在网络中 100% 共享卷积计算的话，我们应该如何在位置不变性（location invariance）和位置可变性（location variance）之间做出权衡呢？

R-FCN 的解决方案：位置敏感分数图

每个位置敏感分数图都代表了一个目标类（object class）的一个相关位置。例如，只要是在图像右上角检测到一只猫，就会激活一个分数图（score map）。而当系统看见左下角出现一辆车时，另一个分数图也将会被激活。本质上来讲，这些分数图都是卷积特征图，它们被训练来识别每个目标的特定部位。

以下是 R-FCN 的工作方式：

1、在输入图像上运行一个 CNN（本例中使用的是 ResNet）。

2、添加一个全卷积层，以生成位置敏感分数图的 score bank。这里应该有 k²(C+1) 个分数图，其中，k²代表切分一个目标的相关位置的数量（比如，3²代表一个 3x3 的空间网格），C+1 代表 C 个类外加一个背景。

3、运行一个全卷积 region proposal 网络（RPN），以生成感兴趣区域（regions of interest，RoI）。

4、对于每个 RoI，我们都将其切分成同样的 k²个子区域，然后将这些子区域作为分数图。

5、对每个子区域，我们检查其 score bank，以判断这个子区域是否匹配具体目标的对应位置。比如，如果我们处在「上-左」子区域，那我们就会获取与这个目标「上-左」子区域对应的分数图，并且在感兴趣区域（RoI region）里对那些值取平均。对每个类我们都要进行这个过程。

6、一旦每个 k²子区域都具备每个类的「目标匹配」值，那么我们就可以对这些子区域求平均值，得到每个类的分数。

7、通过对剩下 C+1 个维度向量进行 softmax 回归，来对 RoI 进行分类。

下面是 R-FCN 的示意图，用 RPN 生成 RoI：

![img](https://image.jiqizhixin.com/uploads/wangeditor/5867511d-e4c0-4d44-995d-17cf551cda31/52256image%20(69).png)

当然，即便有上述文字以及图片的解释，你可能仍然不太明白这个模型的工作方式。老实说，当你可以实际看到 R-FCN 的工作过程时，你会发现理解起来会更加简单。下面就是一个在实践中应用的 R-FCN，它正在从图中检测一个婴儿：

![img](https://image.jiqizhixin.com/uploads/wangeditor/5867511d-e4c0-4d44-995d-17cf551cda31/89128image%20(70).png)

我们只用简单地让 R-FCN 去处理每个 region proposal，然后将其切分成子区域，在子区域上反复询问系统：「这看起来像是婴儿的『上-左』部分吗？」，「这看起来像是婴儿的『上-中』部分吗？」，「这看起来像是婴儿的『上-右』部分吗？」等等。系统会对所有类重复这个过程。如果有足够的子区域表示「是的，我的确匹配婴儿的这个部分！」那么 RoI 就会通过对所有类进行 softmax 回归的方式被分类成一个婴儿。」

借助这种设置，R-FCN 便能同时处理位置可变性（location variance）与位置不变性（location invariance）。它给出不同的目标区域来处理位置可变性，让每个 region proposal 都参考同一个分数图 score bank 来处理位置不变形。这些分数图应该去学习将一只猫分类成猫，而不用管这只猫在在那个位置。最好的是，由于它是全卷积的，所以这意味着网络中所有的计算都是共享的。

因此，R-FCN 比 Faster R-CNN 快了好几倍，并且可以达到类似的准确率。

#### SSD

我们最后一个模型是 SSD，即 Single-Shot Detector。和 R-FCN 一样，它的速度比 Faster R-CNN 要快很多，但其工作方式却和 R-FCN 存在显著不同。

我们前两个模型分两个步骤执行 region proposal 和 region classification。首先，它们使用一个 region proposal 网络来生成感兴趣区域（region of interest）；然后，它们既可以用全连接层也可以用位置敏感卷积层来对那些区域进行分类。然而，SSD 可以在单个步骤中完成上述两个步骤，并且在处理图像的同时预测边界框和类。

具体而言，给定一个输入图像以及一系列真值标签，SSD 就会进行如下操作：

1、在一系列卷积层中传递这个图像，产生一系列大小不同的特征图（比如 10x10、6x6、3x3 等等。）

2、对每个这些特征图中的每个位置而言，都使用一个 3x3 的卷积滤波器（convolutional filter）来评估一小部分默认的边界框。这些默认边的界框本质上等价于 Faster R-CNN 的 anchor box。

3、对每个边界框都同时执行预测： a）边界框的偏移；b）分类的概率。

4、在训练期间，用这些基于 IoU（Intersection over Union，也被称为 Jaccard 相似系数）系数的预测边界框来匹配正确的边界框。被最佳预测的边界框将被标签为「正」，并且其它边界框的 IoU 大于 0.5。

SSD 的工作方式听上去很直接，但是训练它却会面临一个不一般的挑战。在之前那两个模型那里，region proposal 网络可以确保每个我们尝试进行分类的对象都会有一个作为「目标」的最小概率值。然而，在 SSD 这里，我们跳过了这个筛选步骤。我们从图像中每个单一位置那里进行分类并画出形状、大小不同的边界框。通过这种办法，我们可以生成比别的模型更多的边界框，但是它们基本上全是负面样本。

为了解决这个问题，SSD 进行了两项处理。首先，它使用非极大值抑制（non maximum suppression，NMS）技术来将高度重叠的边界框整合成一个。换句话说，如果有 4 个形状、尺寸等类似的边界框中有同一只狗，那么 NMS 就会保留信度最高的那个边界框而抛弃掉其它的。第二，SSD 模型使用了一种被称为 hard negative mining 的技术以在训练过程中让类保持平衡。在 hard negative mining 中，只有那些有最高训练损失（training loss）的负面样本（negative example）子集才会在每次训练迭代中被使用。SSD 的「正负」比一直保持在 1:3。

下图是 SSD 的架构示意图：

![img](https://image.jiqizhixin.com/uploads/wangeditor/5867511d-e4c0-4d44-995d-17cf551cda31/784991-p-lSawysBsiBzlcWZ9_UMw.png)

如上所述，最终有可缩小尺寸的「额外特征层」。这些尺寸变化的特征图有助于捕捉不同大小的目标。例如，下面是一个正在执行的 SSD。

![img](https://image.jiqizhixin.com/uploads/wangeditor/5867511d-e4c0-4d44-995d-17cf551cda31/16545image%20(71).png)

在更小的特征图中（比如 4x4），每一单元覆盖图像的一个更大区域，使其探测更大的目标。region proposal 与分类同时被执行：假设 p 为目标类别，每个边界框与一个 (4+p)-维度向量相连接，其输出 4 个框偏移坐标和 p 分类概率。在最后一步中，softmax 又一次被用来分类目标。

最终，SSD 与最初的两个模型并无不同。它简单跳过「region proposal」这一步，而不是同时考虑图像每个位置的每个边界及其分类。由于 SSD 一次性完成所有，它是三个模型中最快的，且相对而言依然表现出色。

#### YOLO (You Only Look once)

YOLO 是当前深度学习领域解决图像检测问题最先进的实时系统。如下图所示，YOLO 首先将图像划分为规定的边界框，然后对所有边界框并行运行识别算法，来确定物体所属的类别。确定类别之后，yolo 继续智能地合并这些边界框，在物体周围形成最优边界框。

这些步骤全部并行进行，因此 YOLO 能够实现实时运行，并且每秒处理多达 40 张图像。

尽管相比于 RCNN 它的表现有所降低，但在日常实时的问题中它还是有优势的。下图是 YOLO 架构的示图：

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/60842640-13.jpeg)

论文：You Only Look Once: Unified, Real-Time Object Detection

- 链接：https://pjreddie.com/media/files/papers/yolo.pdf
- 代码实现：https://github.com/allanzelener/YAD2K

### 图像分割

#### SegNet 

SegNet 是一个用于解决图像分割问题的深度学习架构。它包含处理层（编码器）序列，之后是对应的解码器序列，用于分类像素。下图是 SegNet 解析图：

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/38884640-16.jpeg)

SegNet 的一个主要特征是在编码器网络的池化指标与解码器网络的池化指标连接时，分割图像保留高频细节。简言之，直接进行信息迁移，而非卷积它们。在处理图像分割问题时，SgeNet 是最好的模型之一。

论文：SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation

- 链接：https://arxiv.org/abs/1511.00561
- 代码实现：https://github.com/imlab-uiip/keras-segnet

#### Mask RCNN

##### Mask-RCNN 介绍

Mask-RCNN 来自于年轻有为的 Kaiming 大神，通过在 Faster-RCNN 的基础上添加一个分支网络，在实现目标检测的同时，把目标像素分割出来。

​       论文下载：[Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)           [部分翻译](http://weixin.niurenqushi.com/article/2017-03-29/4805787.html)

​       代码下载：【[Github](https://github.com/CharlesShang/FastMaskRCNN)】

​       Mask-RCNN 的网络结构示意（在作者原图基础上修改了一下）：

​        ![img](http://img.blog.csdn.net/20170614225558493)

​       假设大家对 Faster 已经很熟悉了，不熟悉的同学建议先看下之前的博文：【[目标检测-RCNN系列](http://blog.csdn.net/linolzhang/article/details/54344350)】

​       其中 黑色部分为原来的 Faster-RCNN，红色部分为在 Faster网络上的修改：

> **1）将 Roi Pooling 层替换成了 RoiAlign；**
>
> **2）添加并列的 FCN 层（mask 层）；**

​       先来概述一下 Mask-RCNN 的几个特点（来自于 Paper 的 Abstract）：

> **1）在边框识别的基础上添加分支网络，用于 语义Mask 识别；**
>
> **2）训练简单，相对于 Faster 仅增加一个小的 Overhead，可以跑到 5FPS；**
>
> **3）可以方便的扩展到其他任务，比如人的姿态估计 等；**
>
> **4）不借助 Trick，在每个任务上，效果优于目前所有的 single-model entries；**
>
> ​     包括 COCO 2016 的Winners。

​        PS：写到这儿提醒一句，建议大家先读一遍 原 Paper，这样再回来看的话会有第二次理解。

##### RCNN行人检测框架

​       基于最早的 Faster RCNN 框架，出现不少改进，主要有三篇需要看：

> **1）作者推荐的这篇**
>
> ​     Speed/accuracy trade-offs for modern convolutional object detectors
>
> ​     **论文下载**【[arxiv](https://arxiv.org/pdf/1611.10012.pdf)】
>
> **2）ResNet**
>
> ​     MSRA也算是作者自己的作品，可以 refer to blog【[ResNet残差网络](http://blog.csdn.net/linolzhang/article/details/71380668)】 
>
> ​     **论文下载**【[arxiv](https://arxiv.org/pdf/1512.03385.pdf)】
>
> **3）FPN**
>
> ​     Feature Pyramid Networks for Object Detection，通过特征金字塔来融合多层特征，实现CNN。
>
> ​     **论文下载**【[arxiv](https://arxiv.org/pdf/1612.03144.pdf)】

​       来看下 后面两种 RCNN 方法与 Mask 结合的示意图（直接贴原图了）：

​        ![img](http://img.blog.csdn.net/20170614225604196)

​       图中灰色部分是 原来的 RCNN 结合 ResNet or FPN 的网络，下面黑色部分为新添加的并联 Mask层，这个图本身与上面的图也没有什么区别，旨在说明作者所提出的Mask RCNN 方法的泛化适应能力 - 可以和多种 RCNN框架结合，表现都不错。

##### Mask-RCNN 技术要点

> **● 技术要点1 - 强化的基础网络**
>
> ​     通过 ResNeXt-101+FPN 用作特征提取网络，达到 state-of-the-art 的效果。
>
> **● 技术要点2 - ROIAlign**
>
> ​     采用 ROIAlign 替代 RoiPooling（改进池化操作）。引入了一个插值过程，先通过双线性插值到14*14，再 pooling到7*7，很大程度上解决了仅通过 Pooling 直接采样带来的 Misalignment 对齐问题。
>
> ​     PS： 虽然 Misalignment 在分类问题上影响并不大，但在 Pixel 级别的 Mask 上会存在较大误差。
>
> ​     后面我们把结果对比贴出来（Table2 c & d），能够看到 ROIAlign 带来较大的改进，可以看到，Stride 越大改进越明显。 
>
> ● 技术要点3 - Loss Function
>
> ​     每个 ROIAlign 对应 K * m^2 维度的输出。**K 对应类别个数，即输出 K 个mask**，m对应 池化分辨率（7*7）。Loss 函数定义：
>
> ​            **Lmask(Cls_k) = Sigmoid (Cls_k)，    平均二值交叉熵 （average binary cross-entropy）Loss，通过逐像素的 Sigmoid 计算得到。**
>
> ​     Why K个mask？通过对每个 Class 对应一个 Mask 可以有效避免类间竞争（其他 Class 不贡献 Loss ）。
>
> ​        ![img](http://img.blog.csdn.net/20170614225609072)
>
> ​     通过结果对比来看（Table2 b），也就是作者所说的 Decouple 解耦，要比多分类 的 Softmax 效果好很多。

##### 对比实验效果

![img](http://img.blog.csdn.net/20170614225614884)

​       另外，作者给出了很多实验分割效果，就不都列了，只贴一张 和 FCIS 的对比图（FCIS 出现了Overlap 的问题）：

​       ![img](http://img.blog.csdn.net/20170519154205488)

##### Mask-RCNN 扩展

​       Mask-RCNN 在姿态估计上的扩展，效果不错，有兴趣的童鞋可以看Paper。

​        ![img](http://img.blog.csdn.net/20170614225629099)

#### Detectron

facebook的目标检测研究平台，实现了像mask r-cnn和retinanet的流行的算法。

### 图像生成

#### GAN

GAN 是神经网络架构中完全不同的类别。GAN 中，一种神经网络用于生成全新的、训练集中未曾有过的图像，但却足够真实。例如，以下是 GAN 工作原理的解析图。

![img](https://image.jiqizhixin.com/uploads/wangeditor/60d8a876-fc40-4f74-88b2-e4d097d4894d/66094640-17.jpeg)

论文：Generative Adversarial Networks

- 链接：https://arxiv.org/abs/1406.2661
- 代码实现：https://github.com/bstriner/keras-adversarial

#### GAN及其变种

[文章名称及地址](https://github.com/hindupuravinash/the-gan-zoo)

### RCNN

### Fast RCNN

### Faster RCNN