## 特征工程常用手段

参考目录：

[Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)

### onehot编码

将一个有N个category的特征进行转换，转换为N列的特征，每列对应一个category。会导致数据变成稀疏格式。
基本用法：大多用于一些线性的算法 
去掉第一列避免共线性 
优点：转换为稀疏格式便于存储 
缺点：对于缺失值没有考虑，且没有考虑变量间的关系。 

### hash编码

对固定长度的数组进行 Onehot 编码 。
可用不同的哈希函数重复操作并 bagging 小凹凸精度的结果 
冲突性可能会降低结果的准确度，也可能提升结果的准确度 
优雅的处理新的变量（比如 new user-agents） 

优点：避免极度稀疏的数据 
缺点：可能会引发冲突性，会有小概率情况下会将不同

### 标签编码

给每个类一个独一无二的数字化 ID 
对于非线性的基于树模型的算法很有用 

优点：不增加维度 
缺点：完全打乱 类型变量(cat_var) -->数字化id(num_id)的映射并重新训练，平均小凹凸精度。 

### 分段编码

将连续型变量按照一定条件（固定距离，相同比例，决策树）切分开，转换为离散型变量。

对于非线性模型有效

优点：对异常值不敏感

缺点：数据差异性减小

### Count 编码 

用训练集里的 count 替换对应的变量 
可以加入对数变换，和 counts 一起使用时效果良好 
用‘1’代替没有考虑的变量 
可能产生的问题：相同的编码，不同的变量 

 优点：对线性或非线性的算法都适用 
缺点：对异常值敏感 

###  LabelCount 编码 

按训练集中的 counts 等级给变量分类 
对线性或非线性算法都适用 
优点：对异常值不敏感 

### RankCount编码

按训练集中count的排名带替换对应变量

对线性或非线性的算法都适用 

优点：对异常值不敏感

### 目标编码 

分类问题中用训练集中特征对目标的比例来进行编码，也即先验概率。回归问题中为变量和/目标和的比例。
注意避免过拟合 
Stacking 的形式：输出目标的平均值的单变量模型 
记得做交叉验证 
加入平滑性避免出现编码为 0 的情况 
加入随机噪声避免过拟合 
当被正确的应用时，是最好的线性或非线性编码 

### 嵌入编码

使用神经网络将类别变量进行嵌入，映射到向量空间中。

优点：比onehot更精准，训练速度快，内存开销小

### NaN编码

对Nan进行填充，使用一个明确的值来代替

注意避免过拟合 ，仅当 NAN 值在训练集和测试集中一致或在本地验证了其独立性时方

可使用 

### 多项式编码





