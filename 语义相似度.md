## 语义相似度

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf

https://cloud.tencent.com/developer/article/1005600

http://ju.outofmemory.cn/entry/316660

### DSSM

DSSM [1]（Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来**预测两个句子的语义相似度**，又可以**获得某句子的低纬语义向量表达**。

DSSM 从下往上可以分为三层结构：**输入层**、**表示层**、**匹配层**

![img](E:\code\python\assets\1501555296606_1048_1501555297548.png)

#### 输入层

输入层做的事情是把句子映射到一个向量空间里并输入到 DNN 中，这里**英文和中文的处理方式有很大的不同**。

**（1）英文**

英文的输入层处理方式是通过**word hashing。**举个例子，假设用 letter-trigams 来切分单词（3 个字母为一组，#表示开始和结束符），boy 这个单词会被切为 #-b-o, b-o-y, o-y-#

![img](E:\code\python\assets\1501555325670_2316_1501555326595.png)

这样做的好处有两个：首先是**压缩空间**，50 万个词的 one-hot 向量空间可以通过 letter-trigram 压缩为一个 3 万维的向量空间。其次是**增强范化能力**，三个字母的表达往往能代表英文中的前缀和后缀，而前缀后缀往往具有通用的语义。

这里之所以用 3 个字母的切分粒度，是综合考虑了**向量空间**和**单词冲突**：

![img](E:\code\python\assets\1501555347709_7385_1501555348626.png)

以 50 万个单词的词库为例，2 个字母的切分粒度的单词冲突为 1192（冲突的定义：至少有两个单词的 letter-bigram 向量完全相同），而 3 个字母的单词冲突降为 22 效果很好，且转化后的向量空间 3 万维不是很大，综合考虑选择 3 个字母的切分粒度。

**（2）中文**

中文的输入层处理方式与英文有很大不同，首先中文**分词**是个让所有 NLP 从业者头疼的事情，即便业界号称能做到 95%左右的分词准确性，但分词结果极为不可控，往往会在分词阶段引入误差。所以这里我们不分词，而是仿照英文的处理方式，对应到中文的最小粒度就是**单字**了。（曾经有人用偏旁部首切的，感兴趣的朋友可以试试）

由于**常用的单字为 1.5 万**左右，而常用的双字大约到百万级别了，所以这里出于向量空间的考虑，采用**字向量**（one-hot）作为输入，向量空间约为 1.5 万维。

#### 表示层

DSSM 的表示层采用 BOW（Bag of words）的方式，相当于把字向量的位置信息抛弃了，整个句子里的词都放在一个袋子里了，不分先后顺序。当然这样做会有问题，我们先为 CNN-DSSM 和 LSTM-DSSM 埋下一个伏笔。

紧接着是一个含有多个隐层的 DNN，如下图所示：

![img](E:\code\python\assets\1501555384122_4617_1501555385245.png)

用 Wi 表示第 i 层的权值矩阵，bi 表示第 i 层的 bias 项。则第一隐层向量 l1（300 维），第 i 个隐层向量 li（300 维），输出向量 y（128 维）可以分别表示为：

![img](E:\code\python\assets\1501555503697_9407_1501555504636.png)

用 tanh 作为隐层和输出层的激活函数：

![img](E:\code\python\assets\1501555521224_301_1501555522121.png)

最终输出一个 128 维的低纬语义向量。

####  匹配层

Query 和 Doc 的语义相似性可以用这**两个语义向量(128 维) 的 cosine 距离**来表示：

![img](E:\code\python\assets\1501555545519_4107_1501555546427.png)

通过**softmax 函数**可以把**Query 与正样本 Doc 的语义相似性**转化为一个**后验概率**：

![img](E:\code\python\assets\1501555590842_9539_1501555591755.png)

其中 r 为 softmax 的平滑因子，D 为 Query 下的正样本，D-为 Query 下的负样本（采取随机负采样），D 为 Query 下的整个样本空间。

在训练阶段，通过**极大似然估计**，我们**最小化损失函数**：

![img](E:\code\python\assets\1501555602634_219_1501555603542.png)

**残差**会在表示层的 DNN 中**反向传播**，最终通过**随机梯度下降（SGD）**使模型收敛，得到各网络层的参数{Wi,bi}。

#### 技巧

#### 优缺点

优点：DSSM 用字向量作为输入既可以**减少切词的依赖**，又可以**提高模型的范化能力**，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用 Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA 的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和 LDA 都是**无监督的训练**，这样**会给整个模型引入误差**，**DSSM 采用统一的有监督训练**，不需要在中间过程做无监督模型的映射，因此精准度会比较高。

缺点：上文提到 DSSM 采用**词袋模型（BOW）**，因此**丧失了语序信息和上下文信息**。另一方面，DSSM 采用**弱监督**、**端到端的模型**，预测结果**不可控**。

### CNN-DSSM

### LSTM-DSSM